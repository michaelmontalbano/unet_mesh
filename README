Here we have a data science project, essentially. 
It includes:

arrange_data.py to interact with the original data source,
and filter it for training samples. Here, we arrange the 
data into a conveniently designed linux filesystem

load_data.py - this... loads the data, taking it from the 
.netcdfs into numpy arrays. Cleans the data and saves it as .npys

u_net.py - contains the code for the unet

basic.py - runs the experiment, activates the model 
and saves the results in a pickle file

display.py - provides visual analysis tools, for human subjective
analysis.

image_analysis/image_metrics.R - loads the python pickle files, which contain the 
all the training information. We use SpatialVx, a spatial verification
package, to collect spatial matching information, and store that in a dataframe 
which is how we score each model. 

Currently, our best model is the unfiltered, original dataset, containing 
3700 samples in total. Additionally, we have found that removing sets of fields,
like NSE, has minor effects on performance. Removing many has a more pronounced effect.
And, in all, the best model is the one that ingests all the features and uses the most samples.

